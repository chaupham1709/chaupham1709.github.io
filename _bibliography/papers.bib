@article{pham2024personalized,
  abbr={CVPRW},
  title={Personalized Large Vision-Language Models},
  author={Pham, Chau and Phan, Hoang and Doermann, David and Tian, Yunjie},
  journal={arXiv preprint arXiv:2412.17610},
  year={2024},
  pdf={https://arxiv.org/abs/2412.17610},
  abstract={The personalization model has gained significant attention in image generation yet remains underexplored for large vision-language models (LVLMs).
  Beyond generic ones, with personalization,
  LVLMs handle interactive dialogues using referential concepts (e.g., ``Mike and Susan are talking.'')
  instead of the generic form (e.g., ``a boy and a girl are talking.''),
  making the conversation more customizable and referentially friendly.
  In addition, PLVM is equipped to continuously add new concepts during a dialogue without incurring additional costs, which significantly enhances the practicality. PLVM proposes Aligner, a pre-trained visual encoder to align referential concepts with the queried images. During the dialogues, it extracts features of reference images with these corresponding concepts and recognizes them in the queried image, enabling personalization. We note that the computational cost and parameter count of the Aligner are negligible within the entire framework. With comprehensive qualitative and quantitative analyses, we reveal the effectiveness and superiority of PLVM.}
}

@article{pham2023lp,
  abbr={WACV},
  title={LP-OVOD: Open-Vocabulary Object Detection by Linear Probing},
  author={Pham*, Chau and Vu*, Truong and Nguyen, Khoi},
  selected = {true},
  journal={Winter Conference on Applications of Computer Vision},
  published={true},
  year={2024},
  pdf={https://arxiv.org/pdf/2310.17109v1.pdf},
  abstract={This paper addresses the challenging problem of openvocabulary object detection (OVOD) where an object detector must identify both seen and unseen classes in test images
without labeled examples of the unseen classes in training.
A typical approach for OVOD is to use joint text-image embeddings of CLIP to assign box proposals to their closest
text label. However, this method has a critical issue: many
low-quality boxes, such as over- and under-covered-object
boxes, have the same similarity score as high-quality boxes
since CLIP is not trained on exact object location information. To address this issue, we propose a novel method, LPOVOD, that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the
top relevant region proposals to the novel text. Experimental results on COCO affirm the superior performance of
our approach over the state of the art, achieving 40.5 in
APnovel using ResNet50 as the backbone and without external datasets or knowing novel classes during training.
Our code will be available at https://github.com/
VinAIResearch/LP-OVOD.}
}

@article{nguyen2022few,
  abbr = {ECCV},
  title={Few-shot object counting and detection},
  author={Nguyen*, Thanh and Pham*, Chau and Nguyen, Khoi and Hoai, Minh},
  selected = {true},
  journal={European Conference on Computer Vision},
  published={true},
  pdf={https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800336.pdf},
  abstract={We tackle a new task of few-shot object counting and detection. Given a few exemplar bounding boxes of a target object class, we seek to count and detect all objects of the target class. This task shares the same supervision as the few-shot object counting but additionally outputs the object bounding boxes along with the total object
count. To address this challenging problem, we introduce a novel twostage training strategy and a novel uncertainty-aware few-shot object
detector: Counting-DETR. The former is aimed at generating pseudo
ground-truth bounding boxes to train the latter. The latter leverages the pseudo ground-truth provided by the former but takes the necessary
steps to account for the imperfection of pseudo ground-truth. To validate the performance of our method on the new task, we introduce two new datasets named FSCD-147 and FSCD-LVIS. Both datasets contain
images with complex scenes, multiple object classes per image, and a
huge variation in object shapes, sizes, and appearance. Our proposed
approach outperforms very strong baselines adapted from few-shot object counting and few-shot object detection with a large margin in both
counting and detection metrics.},
  year={2022}
}
